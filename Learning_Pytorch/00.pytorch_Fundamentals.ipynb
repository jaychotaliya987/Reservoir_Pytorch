{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.tensor` creates the tensor    \n",
    "`TENSOR.item` returns the python type data from the`torch.tensor`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar = torch.tensor(7)\n",
    "\n",
    "vector = torch.tensor([89,69])\n",
    "\n",
    "MATRIX = torch.tensor(\n",
    "    [[12, 21, 12],\n",
    "    [12, 12, 12]]\n",
    ")\n",
    "\n",
    "TENSOR = torch.tensor(\n",
    "    [[[12, 21, 12],\n",
    "    [12, 12, 12],\n",
    "    [23, 4, 334]]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1, 2, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scalar.ndim, vector.ndim, MATRIX.ndim, TENSOR.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SIZE of a tensor - the size of a tensor is such that first number will give us the number of blocks, and following numbers will give us number of sub-blocks and so on, last two numbers will give us the dimension of matrices and 3rd last number will give us the number of matrices in the last sub-block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([]), torch.Size([2]), torch.Size([2, 3]), torch.Size([1, 3, 3]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scalar.shape, vector.shape, MATRIX.shape, TENSOR.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also interestingly there are two functions,   \n",
    "`torch.tensor` which will infer the dtype as is    \n",
    "`torch.Tensor` which is basically `torch.FloatTensor` \n",
    "\n",
    "this is a rabbit of some sort, Tensor is faster then tensor, also I have found out that `torch.Tensor` generates a tensor with that number of random elements.\n",
    "so, `torch.Tensor(7)` will generate a vector with 7 random numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(7),\n",
       " tensor([1.3225e-08, 2.6947e-09, 1.6132e-07, 1.2868e-11, 2.6446e+20, 6.5927e-10,\n",
       "         4.2014e-05]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(7), torch.Tensor(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random tensors\n",
    "Creating random tensors is useful in NNets because we can generate a initial weights and biases randomly. and start from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4025, 0.1842, 0.7614],\n",
       "        [0.2843, 0.3648, 0.7877],\n",
       "        [0.4670, 0.6130, 0.3575]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_tensor = torch.rand(3, 3)\n",
    "random_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Color image tensors are generally of tensor dim = 3, 3 dimensional tensors, each dimensions representing the activation of different color channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([224, 224, 3]), 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_image_tensor = torch.rand( size = (224, 224, 3))\n",
    "random_image_tensor.shape, random_image_tensor.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zeros and Ones\n",
    "zeros and ones tensors for masking and manipulating the tensor via multiplication of summation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros = torch.zeros(3, 3) #? 3x3 matrix of zeros\n",
    "ones = torch.ones(3, 3) #? 3x3 matrix of ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Range Tensors and Tansor-like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 2, 4, 6, 8])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step_tensor = torch.arange(start = 0, end = 10, step = 2) #? 0 to 10 with step 2\n",
    "step_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a zeros tensor but with the shape of a input tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step_tensor_like = torch.zeros_like(input=step_tensor) \n",
    "step_tensor_like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Data Types\n",
    "https://pytorch.org/docs/stable/tensors.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3]), torch.float32, device(type='cpu'))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Default datatype for tensors is float32\n",
    "float_32_tensor = torch.tensor([3.0, 6.0, 9.0],\n",
    "                               dtype=None, #* defaults to None, which is torch.float32 or whatever datatype is passed\n",
    "                               device=None, #* defaults to None, which uses the default tensor type\n",
    "                               requires_grad=False) #* if True, operations performed on the tensor are recorded \n",
    "\n",
    "float_32_tensor.shape, float_32_tensor.dtype, float_32_tensor.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get the information about the tensor by `TENSOR.shpae, TENSOR.dtype, TENSOR.device`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the dtype of the tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float16"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float_16_tensor = float_32_tensor.type(torch.float16) # torch.half would also work\n",
    "float_16_tensor.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor operations\n",
    "All the basic operations works as you expect the only thing you need to be careful is with some linear algebra operations like dot product, hadamard product and so on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Multiplication\n",
    "\n",
    "Dot product in linear algebra, no. of rows of the first tensor should match no. column of second tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.tensor([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.31 ms, sys: 0 ns, total: 3.31 ms\n",
      "Wall time: 2.14 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(14)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Matrix multiplication by hand \n",
    "# (avoid doing operations with for loops at all cost, they are computationally expensive)\n",
    "value = 0\n",
    "for i in range(len(tensor)):\n",
    "  value += tensor[i] * tensor[i]\n",
    "value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 488 µs, sys: 131 µs, total: 619 µs\n",
      "Wall time: 466 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(14)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "torch.matmul(tensor, tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.matmul` is faster than the for-loop. this is a general guidline in all the library function everywhere. **-DO NOT USE RAW LOOPS-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transpose function\n",
    "\n",
    "Can be useful when multiplying the same rectangular tensor.    \n",
    "- `torch.transpose (TENSOR, dim0, dim1)` is a way to transpose in a dimension for 2d the obvious transpose is between 0th dim and 1st dim. In 3d or 4d tensor it will transpose the appropriate dims, for example change 3rd with 4th in 5dim tensor.    \n",
    "- `Tensor.T` is a simplified transpose. always transpose last two dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.2863, 0.3250]],\n",
       " \n",
       "         [[0.8983, 0.2050]],\n",
       " \n",
       "         [[0.6503, 0.7684]]]),\n",
       " tensor([[[0.2863],\n",
       "          [0.3250]],\n",
       " \n",
       "         [[0.8983],\n",
       "          [0.2050]],\n",
       " \n",
       "         [[0.6503],\n",
       "          [0.7684]]]),\n",
       " tensor([[[0.2863],\n",
       "          [0.3250]],\n",
       " \n",
       "         [[0.8983],\n",
       "          [0.2050]],\n",
       " \n",
       "         [[0.6503],\n",
       "          [0.7684]]]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.rand(3, 1, 2)\n",
    "tensor1 = torch.transpose(tensor, 1, 2)\n",
    "tensor, tensor1, tensor.mT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### min, max, mean, sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mean function only work on float type objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.,  2.,  4.,  6.,  8., 10.]), torch.float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(start = 0, end = 11, step = 2).type(torch.float32)\n",
    "x , x.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.), tensor(10.), tensor(5.), tensor(30.))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.min(x), torch.max(x), torch.mean(x), torch.sum(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(5), tensor(0))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(x), torch.argmin(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping, stacking, squeezing, unsqueezing tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often times you'll want to reshape or change the dimensions of your tensors without actually changing the values inside them.\n",
    "\n",
    "To do so, some popular methods are:\n",
    "\n",
    "| Method | One-line description |\n",
    "| ----- | ----- |\n",
    "| [`torch.reshape(input, shape)`](https://pytorch.org/docs/stable/generated/torch.reshape.html#torch.reshape) | Reshapes `input` to `shape` (if compatible), can also use `torch.Tensor.reshape()`. |\n",
    "| [`Tensor.view(shape)`](https://pytorch.org/docs/stable/generated/torch.Tensor.view.html) | Returns a view of the original tensor in a different `shape` but shares the same data as the original tensor. |\n",
    "| [`torch.stack(tensors, dim=0)`](https://pytorch.org/docs/1.9.1/generated/torch.stack.html) | Concatenates a sequence of `tensors` along a new dimension (`dim`), all `tensors` must be same size. |\n",
    "| [`torch.squeeze(input)`](https://pytorch.org/docs/stable/generated/torch.squeeze.html) | Squeezes `input` to remove all the dimenions with value `1`. |\n",
    "| [`torch.unsqueeze(input, dim)`](https://pytorch.org/docs/1.9.1/generated/torch.unsqueeze.html) | Returns `input` with a dimension value of `1` added at `dim`. | \n",
    "| [`torch.permute(input, dims)`](https://pytorch.org/docs/stable/generated/torch.permute.html) | Returns a *view* of the original `input` with its dimensions permuted (rearranged) to `dims`. | \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.,  2.,  4.,  6.,  8., 10.]), torch.Size([6]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(start = 0, end = 11, step = 2).type(torch.float32)\n",
    "x, x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reshape will add or remove the dimensions if compatible, that is total number of the elements must match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  2.,  4.],\n",
       "        [ 6.,  8., 10.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_reshape = x.reshape (2, 3)\n",
    "x_reshape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "view will have the same memory address. so changing the view will change the original tensor, but they are not the same tensors. The dimension of the tensors can be different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 5.,  2.,  4.],\n",
       "         [ 5.,  8., 10.]]),\n",
       " tensor([ 5.,  2.,  4.,  5.,  8., 10.]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = x.view(2, 3)\n",
    "z[:, 0] = 5\n",
    "z , x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 1, 4]), torch.Size([2, 3, 4]), torch.Size([1, 2, 3, 1, 4]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.zeros(2, 3, 1, 4)\n",
    "y = torch.squeeze(x).shape #* removes all dimensions of size 1\n",
    "z = torch.unsqueeze(x, dim=0).shape #* adds a dimension at the specified index\n",
    "x.shape,y,z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy with pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch tensor: tensor([1, 2, 3])\n",
      "NumPy array: [1 2 3]\n",
      "PyTorch tensor (converted back from NumPy array): tensor([1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "# Convert PyTorch tensor to NumPy array\n",
    "tensor = torch.tensor([1, 2, 3])\n",
    "numpy_array = tensor.numpy()\n",
    "\n",
    "# Convert NumPy array back to PyTorch tensor\n",
    "tensor_back = torch.from_numpy(numpy_array)\n",
    "\n",
    "print(\"PyTorch tensor:\", tensor)\n",
    "print(\"NumPy array:\", numpy_array)\n",
    "print(\"PyTorch tensor (converted back from NumPy array):\", tensor_back)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device agonistic code with pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 7, 9])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define tensors\n",
    "tensor1 = torch.tensor([1, 2, 3]).to(device)\n",
    "tensor2 = torch.tensor([4, 5, 6]).to(device)\n",
    "\n",
    "# Perform operations on tensors\n",
    "result = tensor1 + tensor2\n",
    "\n",
    "# Move result back to CPU if necessary\n",
    "result = result.cpu()\n",
    "\n",
    "# Print the result\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
